{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1608bd16",
   "metadata": {},
   "source": [
    "# Advanced Training Techniques - IRST Library\n",
    "## Multi-GPU Training, Hyperparameter Optimization & Custom Losses\n",
    "\n",
    "This notebook demonstrates advanced training techniques for infrared small target detection using the IRST Library.\n",
    "\n",
    "### üéØ **What you'll learn:**\n",
    "- üî• **Multi-GPU Training** - Distributed training across multiple GPUs\n",
    "- üéõÔ∏è **Hyperparameter Optimization** - Automated hyperparameter tuning with Optuna\n",
    "- üß† **Custom Loss Functions** - Design and implement domain-specific losses\n",
    "- üìä **Advanced Metrics** - Comprehensive evaluation with custom metrics\n",
    "- üöÄ **Mixed Precision Training** - Accelerated training with reduced memory usage\n",
    "- üîÑ **Learning Rate Schedules** - Advanced scheduling strategies\n",
    "- üíæ **Model Checkpointing** - Advanced saving and loading strategies\n",
    "- üìà **Experiment Tracking** - MLflow integration for experiment management\n",
    "\n",
    "### üõ†Ô∏è **Prerequisites:**\n",
    "- Completed the basic tutorial\n",
    "- PyTorch with CUDA support\n",
    "- Multiple GPUs (recommended)\n",
    "- Understanding of deep learning fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852f348",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# IRST Library\n",
    "from irst_library import IRSTDetector\n",
    "from irst_library.datasets import SIRSTDataset, IRSTD1kDataset\n",
    "from irst_library.models import SERANKNet, ACMNet, MSHNet\n",
    "from irst_library.training import IRSTTrainer, DistributedTrainer\n",
    "from irst_library.evaluation import IRSTEvaluator\n",
    "from irst_library.utils import visualize_detection, plot_metrics\n",
    "from irst_library.losses import FocalLoss, DiceLoss, IoULoss, TverskyLoss\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Advanced utilities\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up environment\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Use multiple GPUs if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "world_size = torch.cuda.device_count()\n",
    "\n",
    "print(f\"üöÄ Advanced Training Setup Complete!\")\n",
    "print(f\"   üíª Device: {device}\")\n",
    "print(f\"   üî• Available GPUs: {world_size}\")\n",
    "print(f\"   üß† PyTorch Version: {torch.__version__}\")\n",
    "print(f\"   üìä CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Check distributed training capability\n",
    "if world_size > 1:\n",
    "    print(f\"   ‚úÖ Multi-GPU training available with {world_size} GPUs\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Single GPU mode - consider using multiple GPUs for faster training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1574f7",
   "metadata": {},
   "source": [
    "## 2. Multi-GPU Distributed Training\n",
    "\n",
    "Multi-GPU training can significantly speed up training while handling larger batch sizes. Let's set up distributed training with PyTorch's DistributedDataParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_distributed(rank, world_size):\n",
    "    \"\"\"Initialize distributed training.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up distributed training.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class DistributedIRSTTrainer:\n",
    "    \"\"\"Advanced trainer with multi-GPU support.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.device = torch.device(f'cuda:{rank}')\n",
    "        \n",
    "        # Move model to device and wrap with DDP\n",
    "        self.model = model.to(self.device)\n",
    "        self.model = DDP(self.model, device_ids=[rank])\n",
    "        \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        self.scaler = GradScaler() if config.get('mixed_precision', True) else None\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = self.setup_scheduler()\n",
    "        \n",
    "        # Loss functions\n",
    "        self.setup_losses()\n",
    "        \n",
    "        print(f\"üöÄ Distributed trainer initialized on rank {rank}/{world_size}\")\n",
    "    \n",
    "    def setup_scheduler(self):\n",
    "        \"\"\"Setup advanced learning rate scheduler.\"\"\"\n",
    "        if self.config.get('scheduler') == 'onecycle':\n",
    "            return OneCycleLR(\n",
    "                self.optimizer,\n",
    "                max_lr=self.config['learning_rate'],\n",
    "                epochs=self.config['epochs'],\n",
    "                steps_per_epoch=len(self.train_loader)\n",
    "            )\n",
    "        elif self.config.get('scheduler') == 'cosine_restart':\n",
    "            return CosineAnnealingWarmRestarts(\n",
    "                self.optimizer,\n",
    "                T_0=10,\n",
    "                T_mult=2,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "        else:\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=self.config['epochs']\n",
    "            )\n",
    "    \n",
    "    def setup_losses(self):\n",
    "        \"\"\"Setup custom loss functions.\"\"\"\n",
    "        self.losses = {\n",
    "            'focal': FocalLoss(alpha=0.25, gamma=2.0),\n",
    "            'dice': DiceLoss(smooth=1.0),\n",
    "            'iou': IoULoss(),\n",
    "            'tversky': TverskyLoss(alpha=0.7, beta=0.3)\n",
    "        }\n",
    "        self.loss_weights = self.config.get('loss_weights', {\n",
    "            'focal': 0.4, 'dice': 0.3, 'iou': 0.2, 'tversky': 0.1\n",
    "        })\n",
    "\n",
    "# Setup data for distributed training\n",
    "def prepare_distributed_data():\n",
    "    \"\"\"Prepare data loaders for distributed training.\"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    from irst_library.datasets import SIRSTDataset\n",
    "    \n",
    "    # Training transforms with advanced augmentation\n",
    "    train_transform = A.Compose([\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=1.0),\n",
    "            A.Flip(p=1.0),\n",
    "            A.Transpose(p=1.0),\n",
    "        ], p=0.7),\n",
    "        \n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=30,\n",
    "            p=0.6\n",
    "        ),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0)),\n",
    "            A.GaussianBlur(blur_limit=(3, 7)),\n",
    "            A.MotionBlur(blur_limit=7),\n",
    "        ], p=0.5),\n",
    "        \n",
    "        # Advanced infrared-specific augmentations\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n",
    "        \n",
    "        A.Normalize(mean=[0.485], std=[0.229], max_pixel_value=255.0),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={'mask': 'mask'})\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Normalize(mean=[0.485], std=[0.229], max_pixel_value=255.0),\n",
    "        ToTensorV2()\n",
    "    ], additional_targets={'mask': 'mask'})\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SIRSTDataset(\n",
    "        root=\"./data/SIRST\",\n",
    "        split=\"train\",\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = SIRSTDataset(\n",
    "        root=\"./data/SIRST\", \n",
    "        split=\"test\",\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Initialize distributed training if multiple GPUs available\n",
    "if world_size > 1:\n",
    "    print(f\"üî• Setting up distributed training with {world_size} GPUs\")\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset = prepare_distributed_data()\n",
    "    \n",
    "    # Advanced training configuration\n",
    "    distributed_config = {\n",
    "        'epochs': 100,\n",
    "        'batch_size': 16,  # Per GPU batch size\n",
    "        'learning_rate': 2e-4,  # Scaled for multi-GPU\n",
    "        'weight_decay': 1e-4,\n",
    "        'scheduler': 'onecycle',\n",
    "        'mixed_precision': True,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'warmup_epochs': 10,\n",
    "        'loss_weights': {\n",
    "            'focal': 0.4,\n",
    "            'dice': 0.3,\n",
    "            'iou': 0.2,\n",
    "            'tversky': 0.1\n",
    "        },\n",
    "        'save_best_only': True,\n",
    "        'monitor_metric': 'val_iou',\n",
    "        'early_stopping_patience': 15\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Distributed training configuration:\")\n",
    "    for key, value in distributed_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Single GPU mode - distributed training features will be simulated\")\n",
    "    \n",
    "    # Prepare standard datasets for single GPU\n",
    "    train_dataset, val_dataset = prepare_distributed_data()\n",
    "    distributed_config = {\n",
    "        'epochs': 50,\n",
    "        'batch_size': 8,  # Smaller batch for single GPU\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'scheduler': 'cosine',\n",
    "        'mixed_precision': True\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
